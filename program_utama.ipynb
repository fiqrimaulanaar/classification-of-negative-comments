{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import itertools\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>Mas. Sebenarnya aku gamau bully mas tp kek nya...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>Kdang ngaku nicky minaj, kdang beyonce, kdang ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>Time kemas sesuatu tadi, terfikir jugak botol ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>@BasherRL heh kontol, jan sok pemes lu di rp, ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>Maen bacok\"an yuk\"QueenMutiaa: Kalau kamu suka...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6995 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     source  \\\n",
       "0     [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus   \n",
       "1     @verosvante kita2 aja nitizen yang pada kepo,t...  instagram   \n",
       "2     \"#SidangAhok smg sipenista agama n ateknya mat...    twitter   \n",
       "3     @bolususulembang.jkt barusan baca undang2 ini....  instagram   \n",
       "4     bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus   \n",
       "...                                                 ...        ...   \n",
       "6990  Mas. Sebenarnya aku gamau bully mas tp kek nya...  instagram   \n",
       "6991  Kdang ngaku nicky minaj, kdang beyonce, kdang ...  instagram   \n",
       "6992  Time kemas sesuatu tadi, terfikir jugak botol ...    twitter   \n",
       "6993  @BasherRL heh kontol, jan sok pemes lu di rp, ...    twitter   \n",
       "6994  Maen bacok\"an yuk\"QueenMutiaa: Kalau kamu suka...    twitter   \n",
       "\n",
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0              0     0            0                     1  \n",
       "1              0     0            0                     0  \n",
       "2              0     1            1                     1  \n",
       "3              0     0            0                     0  \n",
       "4              0     0            0                     0  \n",
       "...          ...   ...          ...                   ...  \n",
       "6990           0     0            0                     0  \n",
       "6991           0     0            0                     0  \n",
       "6992           0     0            1                     0  \n",
       "6993           1     0            0                     1  \n",
       "6994           0     1            1                     1  \n",
       "\n",
       "[6995 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)-len(df_train.drop_duplicates(subset=['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>Iya miris banget. Smua gara2 pengaruh barat ni...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>Kk luna, harus kuat ya! Harus sabar ngadepin s...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>Mas. Sebenarnya aku gamau bully mas tp kek nya...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>Kdang ngaku nicky minaj, kdang beyonce, kdang ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>Time kemas sesuatu tadi, terfikir jugak botol ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     source  \\\n",
       "0     [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus   \n",
       "1     @verosvante kita2 aja nitizen yang pada kepo,t...  instagram   \n",
       "2     \"#SidangAhok smg sipenista agama n ateknya mat...    twitter   \n",
       "3     @bolususulembang.jkt barusan baca undang2 ini....  instagram   \n",
       "4     bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus   \n",
       "...                                                 ...        ...   \n",
       "6366  Iya miris banget. Smua gara2 pengaruh barat ni...     kaskus   \n",
       "6367  Kk luna, harus kuat ya! Harus sabar ngadepin s...  instagram   \n",
       "6368  Mas. Sebenarnya aku gamau bully mas tp kek nya...  instagram   \n",
       "6369  Kdang ngaku nicky minaj, kdang beyonce, kdang ...  instagram   \n",
       "6370  Time kemas sesuatu tadi, terfikir jugak botol ...    twitter   \n",
       "\n",
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0              0     0            0                     1  \n",
       "1              0     0            0                     0  \n",
       "2              0     1            1                     1  \n",
       "3              0     0            0                     0  \n",
       "4              0     0            0                     0  \n",
       "...          ...   ...          ...                   ...  \n",
       "6366           0     0            0                     0  \n",
       "6367           0     0            0                     0  \n",
       "6368           0     0            0                     0  \n",
       "6369           0     0            0                     0  \n",
       "6370           0     0            1                     0  \n",
       "\n",
       "[6371 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop_duplicates(ignore_index=True, subset=['text'])\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...\n",
       "1       @verosvante kita2 aja nitizen yang pada kepo,t...\n",
       "2       \"#SidangAhok smg sipenista agama n ateknya mat...\n",
       "3       @bolususulembang.jkt barusan baca undang2 ini....\n",
       "4       bikin anak mulu lu nof \\nkaga mikir apa kasian...\n",
       "                              ...                        \n",
       "6366    Iya miris banget. Smua gara2 pengaruh barat ni...\n",
       "6367    Kk luna, harus kuat ya! Harus sabar ngadepin s...\n",
       "6368    Mas. Sebenarnya aku gamau bully mas tp kek nya...\n",
       "6369    Kdang ngaku nicky minaj, kdang beyonce, kdang ...\n",
       "6370    Time kemas sesuatu tadi, terfikir jugak botol ...\n",
       "Name: text, Length: 6371, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukuran train : (6371, 6)\n",
      "ukuran test : (778, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'ukuran train : {df_train.shape}')\n",
    "print(f'ukuran test : {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'source', 'PORNOGRAFI', 'SARA', 'RADIKALISME',\n",
       "       'PENCEMARAN_NAMA_BAIK'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6371"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['SARA'].value_counts().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_train['SARA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['SARA'] == 1].value_counts().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['SARA', 'RADIKALISME', 'PENCEMARAN_NAMA_BAIK', 'PORNOGRAFI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presentase data train untuk label SARA adalah 0.17%\n",
      "presentase data train untuk label RADIKALISME adalah 0.17%\n",
      "presentase data train untuk label PENCEMARAN_NAMA_BAIK adalah 0.31%\n",
      "presentase data train untuk label PORNOGRAFI adalah 0.20%\n"
     ]
    }
   ],
   "source": [
    "for i in labels:\n",
    "    print(\n",
    "        f'presentase data train untuk label {i} adalah {np.sum(df_train[i]) / df_train.value_counts().sum():,.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presentase data test untuk label SARA adalah 0.15%\n",
      "presentase data test untuk label RADIKALISME adalah 0.16%\n",
      "presentase data test untuk label PENCEMARAN_NAMA_BAIK adalah 0.31%\n",
      "presentase data test untuk label PORNOGRAFI adalah 0.22%\n"
     ]
    }
   ],
   "source": [
    "for j in labels:\n",
    "    print(\n",
    "        f'presentase data test untuk label {j} adalah {np.sum(df_test[j]) / df_test.value_counts().sum():,.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, ArrayDictionary, StopWordRemover\n",
    "list_hapus = open(\"list_hapus.txt\", \"r\").read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buat apa  tuh \n"
     ]
    }
   ],
   "source": [
    "def hapus_baris(text):\n",
    "    return re.sub('\\n', ' ',text)\n",
    "\n",
    "kata = 'buat apa \\ntuh\\n'\n",
    "print(hapus_baris(kata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bikin anak mulu lu nof \\nkaga mikir apa kasian anak lu nanti bakalan malu punya bapak kelakuan kaya elu\\r\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hapus_backslash(text):\n",
    "    a = re.sub(r'\\\\+', '/', text)\n",
    "    a = re.sub('/n', '\\n', a)\n",
    "    return a\n",
    "\n",
    "\n",
    "hapus_backslash('bikin anak mulu lu nof \\\\nkaga mikir apa kasian anak lu nanti bakalan malu punya bapak kelakuan kaya elu\\r\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bikin anak mulu lu nof  kaga mikir apa kasian anak lu nanti bakalan malu punya bapak kelakuan kaya elu\\r '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = hapus_backslash(\n",
    "    'bikin anak mulu lu nof \\\\nkaga mikir apa kasian anak lu nanti bakalan malu punya bapak kelakuan kaya elu\\r\\n')\n",
    "a = hapus_baris(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e]yoiii cuy halo halo bandung[/QUOTE]\n",
      "After  :    yoiii cuy halo halo bandung   \n"
     ]
    }
   ],
   "source": [
    "def hapus_quote(text):\n",
    "    text = re.sub('\\[', ' [', text)\n",
    "    text = re.sub('\\]', '] ', text)\n",
    "    text = re.sub('\\[quote[^ ]*\\].*?\\[\\/quote\\]', ' ', text)\n",
    "    text = re.sub('\\[[^ ]*\\]', ' ', text)\n",
    "    text = re.sub('&quot;', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "text = '[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e]yoiii cuy halo halo bandung[/QUOTE]'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', hapus_quote(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemaren kacao bet de www.instagram.com/dj213987qekjhqew\n",
      "After  : kemaren kacao bet de \n"
     ]
    }
   ],
   "source": [
    "def hapus_link(text):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "\n",
    "\n",
    "text = 'kemaren kacao bet de www.instagram.com/dj213987qekjhqew'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', hapus_link(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : budi      pergi ke           pasar\n",
      "After  : budi pergi ke pasar\n"
     ]
    }
   ],
   "source": [
    "def hapus_spasi_banyak(text):\n",
    "    return re.sub('  +', ' ', text)\n",
    "\n",
    "\n",
    "text = 'budi      pergi ke           pasar'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', hapus_spasi_banyak(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemarin,aku pergi ke dagas.terus ketemu sama Ilham.\n",
      "After  : kemarin aku pergi ke dagas terus ketemu sama Ilham\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text, punct=False):\n",
    "    text = WordPunctTokenizer().tokenize(text)\n",
    "    text = [word for word in text if punct or word.isalnum()]\n",
    "    text = ' '.join(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "text = 'kemarin,aku pergi ke dagas.terus ketemu sama Ilham.'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', tokenize_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7an</td>\n",
       "      <td>tujuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@</td>\n",
       "      <td>di</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ababil</td>\n",
       "      <td>abg labil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acc</td>\n",
       "      <td>accord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>tauge</td>\n",
       "      <td>taoge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>toge</td>\n",
       "      <td>taoge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>taubat</td>\n",
       "      <td>tobat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>trilyun</td>\n",
       "      <td>triliun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>vissi</td>\n",
       "      <td>visi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2878 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     original translated\n",
       "0         7an     tujuan\n",
       "1           @         di\n",
       "2      ababil  abg labil\n",
       "3        abis      habis\n",
       "4         acc     accord\n",
       "...       ...        ...\n",
       "2873    tauge      taoge\n",
       "2874     toge      taoge\n",
       "2875   taubat      tobat\n",
       "2876  trilyun    triliun\n",
       "2877    vissi       visi\n",
       "\n",
       "[2878 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang = pd.read_csv('data/slangword.csv')\n",
    "slang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = dict(zip(slang['original'], slang['translated']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : dikit lagi abis nih\n",
      "After  : dikit lagi habis ini\n"
     ]
    }
   ],
   "source": [
    "def transform_slang_words(text):\n",
    "    word_list = text.split()\n",
    "    word_list_len = len(word_list)\n",
    "    transformed_word_list = []\n",
    "    i = 0\n",
    "    while i < word_list_len:\n",
    "        if (i + 1) < word_list_len:\n",
    "            two_words = ' '.join(word_list[i:i+2])\n",
    "            if two_words in slang_dict:\n",
    "                transformed_word_list.append(slang_dict[two_words])\n",
    "                i += 2\n",
    "                continue\n",
    "        transformed_word_list.append(\n",
    "            slang_dict.get(word_list[i], word_list[i]))\n",
    "        i += 1\n",
    "    return ' '.join(transformed_word_list)\n",
    "\n",
    "\n",
    "text = 'dikit lagi abis nih'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', transform_slang_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemaren tu123 ada kelinci di kebun.\n",
      "After  : kemaren tu ada kelinci di kebun\n"
     ]
    }
   ],
   "source": [
    "def remove_non_alphabet(text):\n",
    "    output = re.sub('[^a-zA-Z ]+', '', text)\n",
    "    return output\n",
    "\n",
    "\n",
    "text = 'kemaren tu123 ada kelinci di kebun.'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', remove_non_alphabet(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : @jonijon menurut saya hal tersebut masih kurang baik dilakukan sih kak\n",
      "After  :  menurut saya hal tersebut masih kurang baik dilakukan sih kak\n"
     ]
    }
   ],
   "source": [
    "def remove_twitter_ig_formatting(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "text = '@jonijon menurut saya hal tersebut masih kurang baik dilakukan sih kak'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', remove_twitter_ig_formatting(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', 'jika', 'jika', 'sehingga', 'kembali', 'dan', 'tidak', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', 'setelah', 'belum', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'tanpa', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'ingin', 'juga', 'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun']\n"
     ]
    }
   ],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\n",
      "After  : Andi kerap melakukan transaksi rutin daring online. Menurut Andi belanja online lebih praktis & murah.\n"
     ]
    }
   ],
   "source": [
    "removeFactory = StopWordRemoverFactory()\n",
    "stopwords = removeFactory.create_stop_word_remover()\n",
    "stopwords_plus = removeFactory.get_stop_words()+list_hapus\n",
    "dictionary = ArrayDictionary(stopwords_plus)\n",
    "\n",
    "stopwords = StopWordRemover(dictionary)\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stop = stopwords.remove(text)\n",
    "    return stop\n",
    "\n",
    "\n",
    "text = 'Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.'\n",
    "print('Before :', text)\n",
    "print('After  :', removeStopWords(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\n",
      "After  : andi kerap laku transaksi rutin cara daring atau online turut andi belanja online lebih praktis murah\n"
     ]
    }
   ],
   "source": [
    "stemFactory = StemmerFactory()\n",
    "stemmer = stemFactory.create_stemmer()\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    hasil = stemmer.stem(text)\n",
    "    return hasil\n",
    "\n",
    "\n",
    "text = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', stemming(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : heyyyyyyyyyyyyyyyyyyyy kenapa tadi?\n",
      "After  : hey kenapa tadi?\n"
     ]
    }
   ],
   "source": [
    "def remove_repeating_characters(text):\n",
    "    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n",
    "\n",
    "\n",
    "text = 'heyyyyyyyyyyyyyyyyyyyy kenapa tadi?'\n",
    "\n",
    "print('Before :', text)\n",
    "print('After  :', remove_repeating_characters(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = hapus_quote(text)\n",
    "    text = hapus_backslash(text)\n",
    "    text = hapus_baris(text)\n",
    "    # text = hapus_quote(text)\n",
    "    text = hapus_link(text)\n",
    "    text = remove_twitter_ig_formatting(text)\n",
    "    text = tokenize_text(text)\n",
    "    text = remove_repeating_characters(text)\n",
    "    # text = remove_non_alphabet(text)\n",
    "    text = transform_slang_words(text)\n",
    "    text = remove_non_alphabet(text)\n",
    "    text = hapus_spasi_banyak(text)\n",
    "    text = removeStopWords(text)\n",
    "    # text = stemming(text)\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buat anak melulu kamu nof mikir apa kasian anak kamu akan malu punya bapak kelakuan kaya kamu\n"
     ]
    }
   ],
   "source": [
    "# print(preprocessing('bikin anak mulu lu nof \\\\nkaga mikir apa kasian anak lu nanti bakalan malu punya bapak kelakuan kaya elu\\r\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3976\\451738679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['text'] = df_train['text'].apply(preprocessing)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woi anjing bodoh provinsi paling banyak ngerus...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saja nitizen penasaran keluarga besar sudah pa...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok semoga sipenista agama ateknya mati...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang tetap dibedakan ko...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buat anak melulu kamu nof mikir apa kasian ana...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>iya miris sekali semua gara pengaruh barat sep...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>luna kuat iya sabar ngadepin sikap pembenci mu...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>mas sebenarnya aku mau buly mas seperti nya ma...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>kdang mengaku nicky minaj kdang beyonce kdang ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>waktu kemas tadi terfikir jugak botol air keci...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     source  \\\n",
       "0     woi anjing bodoh provinsi paling banyak ngerus...     kaskus   \n",
       "1     saja nitizen penasaran keluarga besar sudah pa...  instagram   \n",
       "2     sidangahok semoga sipenista agama ateknya mati...    twitter   \n",
       "3     jakarta barusan baca undang tetap dibedakan ko...  instagram   \n",
       "4     buat anak melulu kamu nof mikir apa kasian ana...     kaskus   \n",
       "...                                                 ...        ...   \n",
       "6366  iya miris sekali semua gara pengaruh barat sep...     kaskus   \n",
       "6367  luna kuat iya sabar ngadepin sikap pembenci mu...  instagram   \n",
       "6368  mas sebenarnya aku mau buly mas seperti nya ma...  instagram   \n",
       "6369  kdang mengaku nicky minaj kdang beyonce kdang ...  instagram   \n",
       "6370  waktu kemas tadi terfikir jugak botol air keci...    twitter   \n",
       "\n",
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0              0     0            0                     1  \n",
       "1              0     0            0                     0  \n",
       "2              0     1            1                     1  \n",
       "3              0     0            0                     0  \n",
       "4              0     0            0                     0  \n",
       "...          ...   ...          ...                   ...  \n",
       "6366           0     0            0                     0  \n",
       "6367           0     0            0                     0  \n",
       "6368           0     0            0                     0  \n",
       "6369           0     0            0                     0  \n",
       "6370           0     0            1                     0  \n",
       "\n",
       "[6371 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'] = df_train['text'].apply(preprocessing)\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[QUOTE=Jalan Cinta;5a655723a2c06ebd158b456d]\\\\n\\\\n”Menurut pamannya, pria tersebut tidak waras setelah mengalami kecelakaan lalu lintas 15 tahun yang lalu,” kata Nik Ros Azhan, seperti dikutip dari Bernama, Senin (22/1/2018). Polisi, ujar dia, akan terus melacaknya.\\\\n\\\\nTsnya ikutan gila :travel\\\\n\\\\n[/QUOTE]\\\\nanjing lu goblog, lebih pilih bela malingsia daripada wanita bangsa sendiiri.\\\\nketemu gw injek muka lu sekeluarga \\\\ndasar pengkhianat bangsa:batabig \\\\n\\r\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'][6301]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woi anjing bodoh provinsi paling banyak ngerus...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saja nitizen penasaran keluarga besar sudah pa...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok semoga sipenista agama ateknya mati...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang tetap dibedakan ko...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buat anak melulu kamu nof mikir apa kasian ana...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>iya miris sekali semua gara pengaruh barat sep...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>luna kuat iya sabar ngadepin sikap pembenci mu...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>mas sebenarnya aku mau buly mas seperti nya ma...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>kdang mengaku nicky minaj kdang beyonce kdang ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>waktu kemas tadi terfikir jugak botol air keci...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     source  \\\n",
       "0     woi anjing bodoh provinsi paling banyak ngerus...     kaskus   \n",
       "1     saja nitizen penasaran keluarga besar sudah pa...  instagram   \n",
       "2     sidangahok semoga sipenista agama ateknya mati...    twitter   \n",
       "3     jakarta barusan baca undang tetap dibedakan ko...  instagram   \n",
       "4     buat anak melulu kamu nof mikir apa kasian ana...     kaskus   \n",
       "...                                                 ...        ...   \n",
       "6366  iya miris sekali semua gara pengaruh barat sep...     kaskus   \n",
       "6367  luna kuat iya sabar ngadepin sikap pembenci mu...  instagram   \n",
       "6368  mas sebenarnya aku mau buly mas seperti nya ma...  instagram   \n",
       "6369  kdang mengaku nicky minaj kdang beyonce kdang ...  instagram   \n",
       "6370  waktu kemas tadi terfikir jugak botol air keci...    twitter   \n",
       "\n",
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0              0     0            0                     1  \n",
       "1              0     0            0                     0  \n",
       "2              0     1            1                     1  \n",
       "3              0     0            0                     0  \n",
       "4              0     0            0                     0  \n",
       "...          ...   ...          ...                   ...  \n",
       "6366           0     0            0                     0  \n",
       "6367           0     0            0                     0  \n",
       "6368           0     0            0                     0  \n",
       "6369           0     0            0                     0  \n",
       "6370           0     0            1                     0  \n",
       "\n",
       "[6371 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data_hasil_baru/kalimat_no_stem_addStopwords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       woi anjing bodoh provinsi paling banyak ngerus...\n",
       "1       saja nitizen penasaran keluarga besar sudah pa...\n",
       "2       sidangahok semoga sipenista agama ateknya mati...\n",
       "3       jakarta barusan baca undang tetap dibedakan ko...\n",
       "4       buat anak melulu kamu nof mikir apa kasian ana...\n",
       "                              ...                        \n",
       "6366    iya miris sekali semua gara pengaruh barat sep...\n",
       "6367    luna kuat iya sabar ngadepin sikap pembenci mu...\n",
       "6368    mas sebenarnya aku mau buly mas seperti nya ma...\n",
       "6369    kdang mengaku nicky minaj kdang beyonce kdang ...\n",
       "6370    waktu kemas tadi terfikir jugak botol air keci...\n",
       "Name: text, Length: 6371, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = vectorizer.fit_transform(df_train['text']).toarray()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abadinya</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>abaikan</th>\n",
       "      <th>abang</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abas</th>\n",
       "      <th>...</th>\n",
       "      <th>zona</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 25664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ab  abad  abadi  abadinya  abah  abai  abaikan  abang  abangan  abas  \\\n",
       "0     0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "1     0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "2     0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "3     0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "4     0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "...   ...   ...    ...       ...   ...   ...      ...    ...      ...   ...   \n",
       "6366  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "6367  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "6368  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "6369  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "6370  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0   \n",
       "\n",
       "      ...  zona  zoroaster  zouma  zoya  zuhur  zulkarnaen  zulkarnain  \\\n",
       "0     ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "1     ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "2     ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "3     ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "4     ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "...   ...   ...        ...    ...   ...    ...         ...         ...   \n",
       "6366  ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "6367  ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "6368  ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "6369  ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "6370  ...   0.0        0.0    0.0   0.0    0.0         0.0         0.0   \n",
       "\n",
       "      zulkifli   zy  zynga  \n",
       "0          0.0  0.0    0.0  \n",
       "1          0.0  0.0    0.0  \n",
       "2          0.0  0.0    0.0  \n",
       "3          0.0  0.0    0.0  \n",
       "4          0.0  0.0    0.0  \n",
       "...        ...  ...    ...  \n",
       "6366       0.0  0.0    0.0  \n",
       "6367       0.0  0.0    0.0  \n",
       "6368       0.0  0.0    0.0  \n",
       "6369       0.0  0.0    0.0  \n",
       "6370       0.0  0.0    0.0  \n",
       "\n",
       "[6371 rows x 25664 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baru = pd.DataFrame(result, columns=vectorizer.get_feature_names_out())\n",
    "df_baru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abadinya</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>...</th>\n",
       "      <th>zona</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zoya</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 25668 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK   ab  abad  abadi  \\\n",
       "0              0     0            0                     1  0.0   0.0    0.0   \n",
       "1              0     0            0                     0  0.0   0.0    0.0   \n",
       "2              0     1            1                     1  0.0   0.0    0.0   \n",
       "3              0     0            0                     0  0.0   0.0    0.0   \n",
       "4              0     0            0                     0  0.0   0.0    0.0   \n",
       "...          ...   ...          ...                   ...  ...   ...    ...   \n",
       "6366           0     0            0                     0  0.0   0.0    0.0   \n",
       "6367           0     0            0                     0  0.0   0.0    0.0   \n",
       "6368           0     0            0                     0  0.0   0.0    0.0   \n",
       "6369           0     0            0                     0  0.0   0.0    0.0   \n",
       "6370           0     0            1                     0  0.0   0.0    0.0   \n",
       "\n",
       "      abadinya  abah  abai  ...  zona  zoroaster  zouma  zoya  zuhur  \\\n",
       "0          0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "1          0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "2          0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "3          0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "4          0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "...        ...   ...   ...  ...   ...        ...    ...   ...    ...   \n",
       "6366       0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "6367       0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "6368       0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "6369       0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "6370       0.0   0.0   0.0  ...   0.0        0.0    0.0   0.0    0.0   \n",
       "\n",
       "      zulkarnaen  zulkarnain  zulkifli   zy  zynga  \n",
       "0            0.0         0.0       0.0  0.0    0.0  \n",
       "1            0.0         0.0       0.0  0.0    0.0  \n",
       "2            0.0         0.0       0.0  0.0    0.0  \n",
       "3            0.0         0.0       0.0  0.0    0.0  \n",
       "4            0.0         0.0       0.0  0.0    0.0  \n",
       "...          ...         ...       ...  ...    ...  \n",
       "6366         0.0         0.0       0.0  0.0    0.0  \n",
       "6367         0.0         0.0       0.0  0.0    0.0  \n",
       "6368         0.0         0.0       0.0  0.0    0.0  \n",
       "6369         0.0         0.0       0.0  0.0    0.0  \n",
       "6370         0.0         0.0       0.0  0.0    0.0  \n",
       "\n",
       "[6371 rows x 25668 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_train[['PORNOGRAFI', 'SARA', 'RADIKALISME',\n",
    "               'PENCEMARAN_NAMA_BAIK']], df_baru], axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data fix/preprocess_fix_backslash_wo_stemming.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abadinya</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6995 rows × 25708 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK   ab  abad  abadi  \\\n",
       "0              0     0            0                     1  0.0   0.0    0.0   \n",
       "1              0     0            0                     0  0.0   0.0    0.0   \n",
       "2              0     1            1                     1  0.0   0.0    0.0   \n",
       "3              0     0            0                     0  0.0   0.0    0.0   \n",
       "4              0     0            0                     0  0.0   0.0    0.0   \n",
       "...          ...   ...          ...                   ...  ...   ...    ...   \n",
       "6990           0     0            0                     0  0.0   0.0    0.0   \n",
       "6991           0     0            0                     0  0.0   0.0    0.0   \n",
       "6992           0     0            1                     0  0.0   0.0    0.0   \n",
       "6993           1     0            0                     1  0.0   0.0    0.0   \n",
       "6994           0     1            1                     1  0.0   0.0    0.0   \n",
       "\n",
       "      abadinya  abah  abai  ...  zuhur  zulkarnaen  zulkarnain  zulkifli  \\\n",
       "0          0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "1          0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "2          0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "3          0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "4          0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "...        ...   ...   ...  ...    ...         ...         ...       ...   \n",
       "6990       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "6991       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "6992       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "6993       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "6994       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "\n",
       "      zuqfaiugqy  zvjqohw  zwcxnxhd  zwnbzkjn   zy  zynga  \n",
       "0            0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "1            0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "2            0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "3            0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "4            0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "...          ...      ...       ...       ...  ...    ...  \n",
       "6990         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "6991         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "6992         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "6993         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "6994         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "\n",
       "[6995 rows x 25708 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data hasil/preprocess_wo_stemming_fix_backslash.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abadinya</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>abaikan</th>\n",
       "      <th>abang</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abas</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abad  abadi  abadinya  abah  abai  abaikan  abang  abangan  abas  ...  \\\n",
       "0  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0  ...   \n",
       "1  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0  ...   \n",
       "2  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0  ...   \n",
       "3  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0  ...   \n",
       "4  0.0   0.0    0.0       0.0   0.0   0.0      0.0    0.0      0.0   0.0  ...   \n",
       "\n",
       "   zuhur  zulkarnaen  zulkarnain  zulkifli  zuqfaiugqy  zvjqohw  zwcxnxhd  \\\n",
       "0    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "1    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "2    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "3    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "4    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "\n",
       "   zwnbzkjn   zy  zynga  \n",
       "0       0.0  0.0    0.0  \n",
       "1       0.0  0.0    0.0  \n",
       "2       0.0  0.0    0.0  \n",
       "3       0.0  0.0    0.0  \n",
       "4       0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 25704 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(columns=['PORNOGRAFI', 'SARA',\n",
    "            'RADIKALISME', 'PENCEMARAN_NAMA_BAIK'])\n",
    "x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaceab</th>\n",
       "      <th>aacebdbd</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>abang</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abas</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21586 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaceab  aacebdbd   ab  abad  abadi  abah  abai  abang  abangan  abas  ...  \\\n",
       "0     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "1     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "2     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "3     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "4     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "\n",
       "   zuhur  zulkarnaen  zulkarnain  zulkifli  zuqfaiugqy  zvjqohw  zwcxnxhd  \\\n",
       "0    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "1    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "2    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "3    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "4    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "\n",
       "   zwnbzkjn   zy  zynga  \n",
       "0       0.0  0.0    0.0  \n",
       "1       0.0  0.0    0.0  \n",
       "2       0.0  0.0    0.0  \n",
       "3       0.0  0.0    0.0  \n",
       "4       0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 21586 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = df_baru\n",
    "# x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: PORNOGRAFI, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['PORNOGRAFI']\n",
    "y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, stratify=y, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9543\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(f'Akurasi : {accuracy_score(y_test, y_pred):,.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "print((y_pred == 0).sum())\n",
    "print((y_pred == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1084\n",
      "           1       0.98      0.82      0.89       315\n",
      "\n",
      "    accuracy                           0.95      1399\n",
      "   macro avg       0.96      0.91      0.93      1399\n",
      "weighted avg       0.96      0.95      0.95      1399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil rata-rata akurasi : 0.9512139346355164\n",
      "Rata-rata std : 0.006240749238835373\n",
      "Akurasi agar tidak overfit / underfit 0.9450 - 0.9575 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Skfold = StratifiedKFold(n_splits=5)\n",
    "acc = cross_val_score(model, x_train, y_train, cv=Skfold)\n",
    "print(f'Hasil rata-rata akurasi : {acc.mean()}')\n",
    "print(f'Rata-rata std : {acc.std()}')\n",
    "print(\n",
    "    f'Akurasi agar tidak overfit / underfit {(acc.mean() - acc.std()):,.4f} - {(acc.mean() + acc.std()):,.4f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1078,    6],\n",
       "       [  58,  257]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(y_test, y_pred)\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 110,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9421\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "pred_grid = best_grid.predict(x_test)\n",
    "\n",
    "print(f'Akurasi : {accuracy_score(y_test, pred_grid):,.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=110, min_samples_leaf=3, min_samples_split=8,\n",
       "                       n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=110, min_samples_leaf=3, min_samples_split=8,\n",
       "                       n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=110, min_samples_leaf=3, min_samples_split=8,\n",
       "                       n_estimators=200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "print((pred_grid == 0).sum())\n",
    "print((pred_grid == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil rata-rata akurasi : 0.9366\n",
      "Rata-rata std : 0.0064\n",
      "Akurasi agar tidak overfit / underfit 0.9302 - 0.9429 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Skfold = StratifiedKFold(n_splits=5)\n",
    "acc = cross_val_score(best_grid, x_train, y_train, cv=Skfold)\n",
    "print(f'Hasil rata-rata akurasi : {acc.mean():,.4f}')\n",
    "print(f'Rata-rata std : {acc.std():,.4f}')\n",
    "print(\n",
    "    f'Akurasi agar tidak overfit / underfit {(acc.mean() - acc.std()):,.4f} - {(acc.mean() + acc.std()):,.4f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1081,    3],\n",
       "       [  78,  237]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(y_test, pred_grid)\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1084\n",
      "           1       0.99      0.75      0.85       315\n",
      "\n",
      "    accuracy                           0.94      1399\n",
      "   macro avg       0.96      0.87      0.91      1399\n",
      "weighted avg       0.95      0.94      0.94      1399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After : Counter({0: 5421, 1: 5421})\n"
     ]
    }
   ],
   "source": [
    "oversample = SMOTE()\n",
    "x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "\n",
    "counter = Counter(y_smote)\n",
    "print(f'After : {counter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "    x_smote, y_smote, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9766\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_smote = rf.predict(x_test_smote)\n",
    "\n",
    "acc_tree1 = accuracy_score(y_test_smote, y_pred_smote)\n",
    "\n",
    "print(\n",
    "    f'Akurasi : {accuracy_score(y_test_smote, y_pred_smote):,.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1586,   12],\n",
       "       [  64, 1591]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(y_test_smote, y_pred_smote)\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1598\n",
      "           1       0.99      0.96      0.98      1655\n",
      "\n",
      "    accuracy                           0.98      3253\n",
      "   macro avg       0.98      0.98      0.98      3253\n",
      "weighted avg       0.98      0.98      0.98      3253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test_smote, y_pred_smote)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil rata-rata akurasi : 0.9789\n",
      "Rata-rata std : 0.0065\n",
      "Akurasi agar tidak overfit / underfit 0.9724 - 0.9854 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Skfold = StratifiedKFold(n_splits=5)\n",
    "acc = cross_val_score(rf, x_train_smote, y_train_smote, cv=Skfold)\n",
    "print(f'Hasil rata-rata akurasi : {acc.mean():,.4f}')\n",
    "print(f'Rata-rata std : {acc.std():,.4f}')\n",
    "print(\n",
    "    f'Akurasi agar tidak overfit / underfit {(acc.mean() - acc.std()):,.4f} - {(acc.mean() + acc.std()):,.4f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\venv\\nlp\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 110,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 12,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9757\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "pred_grid = best_grid.predict(x_test)\n",
    "\n",
    "print(f'Akurasi : {accuracy_score(y_test, pred_grid):,.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1587,   11],\n",
       "       [  61, 1594]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(y_test_smote, y_pred_smote)\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1598\n",
      "           1       0.99      0.96      0.98      1655\n",
      "\n",
      "    accuracy                           0.98      3253\n",
      "   macro avg       0.98      0.98      0.98      3253\n",
      "weighted avg       0.98      0.98      0.98      3253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test_smote, y_pred_smote)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latih(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(f'Akurasi : {accuracy_score(y_test, y_pred):,.4f} \\n')\n",
    "    print('---------- Classification Report ----------')\n",
    "    print(f'{classification_report(y_test, y_pred)}\\n')\n",
    "    print('---------- Confusion Matrix ----------')\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latih_hyperparameter(model, x_train, y_train, x_test, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                               cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(f'Parameter terbaik : {grid_search.best_params_} \\n')\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    best_grid.fit(x_train, y_train)\n",
    "    y_pred = best_grid.predict(x_test)\n",
    "    print(f'Akurasi : {accuracy_score(y_test, y_pred):,.4f}')\n",
    "    print('---------- Classification Report ----------')\n",
    "    print(f'{classification_report(y_test, y_pred)}\\n')\n",
    "    print('---------- Confusion Matrix ----------')\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    print(conf)\n",
    "    \n",
    "    return best_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, x_train, y_train):\n",
    "    Skfold = StratifiedKFold(n_splits=5)\n",
    "    acc = cross_val_score(model, x_train, y_train, cv=Skfold)\n",
    "    print(f'Hasil rata-rata akurasi : {acc.mean()}')\n",
    "    print(f'Rata-rata std : {acc.std()}')\n",
    "    print(\n",
    "        f'Akurasi agar tidak overfit / underfit {(acc.mean() - acc.std()):,.4f} - {(acc.mean() + acc.std()):,.4f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9550 \n",
      "\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1084\n",
      "           1       0.97      0.83      0.89       315\n",
      "\n",
      "    accuracy                           0.95      1399\n",
      "   macro avg       0.96      0.91      0.93      1399\n",
      "weighted avg       0.96      0.95      0.95      1399\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1076    8]\n",
      " [  55  260]]\n"
     ]
    }
   ],
   "source": [
    "latih(RandomForestClassifier(), x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9806 \n",
      "\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1598\n",
      "           1       1.00      0.97      0.98      1655\n",
      "\n",
      "    accuracy                           0.98      3253\n",
      "   macro avg       0.98      0.98      0.98      3253\n",
      "weighted avg       0.98      0.98      0.98      3253\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1590    8]\n",
      " [  55 1600]]\n"
     ]
    }
   ],
   "source": [
    "latih(RandomForestClassifier(), x_train_smote,\n",
    "      y_train_smote, x_test_smote, y_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 100} \n",
      "\n",
      "Akurasi : 0.9421\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1084\n",
      "           1       0.99      0.75      0.85       315\n",
      "\n",
      "    accuracy                           0.94      1399\n",
      "   macro avg       0.96      0.87      0.91      1399\n",
      "weighted avg       0.95      0.94      0.94      1399\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1081    3]\n",
      " [  78  237]]\n"
     ]
    }
   ],
   "source": [
    "best_model = latih_hyperparameter(RandomForestClassifier(), x_train,\n",
    "                     y_train, x_test, y_test, param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 200} \n",
      "\n",
      "Akurasi : 0.9671\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1598\n",
      "           1       0.99      0.95      0.97      1655\n",
      "\n",
      "    accuracy                           0.97      3253\n",
      "   macro avg       0.97      0.97      0.97      3253\n",
      "weighted avg       0.97      0.97      0.97      3253\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1579   19]\n",
      " [  88 1567]]\n"
     ]
    }
   ],
   "source": [
    "best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\n",
    "                     y_train_smote, x_test_smote, y_test_smote, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "      <th>aaceab</th>\n",
       "      <th>aacebdbd</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abah</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19896 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  aaceab  aacebdbd   ab  \\\n",
       "0           0     0            0                     1     0.0       0.0  0.0   \n",
       "1           0     0            0                     0     0.0       0.0  0.0   \n",
       "2           0     1            1                     1     0.0       0.0  0.0   \n",
       "3           0     0            0                     0     0.0       0.0  0.0   \n",
       "4           0     0            0                     0     0.0       0.0  0.0   \n",
       "\n",
       "   abad  abadi  abah  ...  zuhur  zulkarnaen  zulkarnain  zulkifli  \\\n",
       "0   0.0    0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "1   0.0    0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "2   0.0    0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "3   0.0    0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "4   0.0    0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "\n",
       "   zuqfaiugqy  zvjqohw  zwcxnxhd  zwnbzkjn   zy  zynga  \n",
       "0         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "1         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "2         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "3         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "4         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 19896 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemming = pd.read_csv('preprocess_fix_backslash.csv')\n",
    "df_stemming.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaceab</th>\n",
       "      <th>aacebdbd</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>abang</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abas</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19892 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaceab  aacebdbd   ab  abad  abadi  abah  abai  abang  abangan  abas  ...  \\\n",
       "0     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "1     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "2     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "3     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "4     0.0       0.0  0.0   0.0    0.0   0.0   0.0    0.0      0.0   0.0  ...   \n",
       "\n",
       "   zuhur  zulkarnaen  zulkarnain  zulkifli  zuqfaiugqy  zvjqohw  zwcxnxhd  \\\n",
       "0    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "1    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "2    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "3    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "4    0.0         0.0         0.0       0.0         0.0      0.0       0.0   \n",
       "\n",
       "   zwnbzkjn   zy  zynga  \n",
       "0       0.0  0.0    0.0  \n",
       "1       0.0  0.0    0.0  \n",
       "2       0.0  0.0    0.0  \n",
       "3       0.0  0.0    0.0  \n",
       "4       0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 19892 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stem = df_stemming.drop(columns=['PORNOGRAFI', 'SARA',\n",
    "                                    'RADIKALISME', 'PENCEMARAN_NAMA_BAIK'])\n",
    "x_stem.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: PORNOGRAFI, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_stem = df_stemming['PORNOGRAFI']\n",
    "y_stem.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After : Counter({0: 5421, 1: 5421})\n"
     ]
    }
   ],
   "source": [
    "oversample = SMOTE()\n",
    "x_stem_smote, y_stem_smote = oversample.fit_resample(x_stem, y_stem)\n",
    "\n",
    "counter = Counter(y_stem_smote)\n",
    "print(f'After : {counter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10842, 19892)\n",
      "(10842,)\n"
     ]
    }
   ],
   "source": [
    "print(x_stem_smote.shape)\n",
    "print(y_stem_smote.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stem_train, x_stem_test, y_stem_train, y_stem_test = train_test_split(\n",
    "    x_stem, y_stem, stratify=y_stem, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stem_train_smote, x_stem_test_smote, y_stem_train_smote, y_stem_test_smote = train_test_split(\n",
    "    x_stem_smote, y_stem_smote, stratify=y_stem_smote, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9528 \n",
      "\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1084\n",
      "           1       0.98      0.81      0.89       315\n",
      "\n",
      "    accuracy                           0.95      1399\n",
      "   macro avg       0.96      0.90      0.93      1399\n",
      "weighted avg       0.95      0.95      0.95      1399\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1078    6]\n",
      " [  60  255]]\n"
     ]
    }
   ],
   "source": [
    "latih(RandomForestClassifier(), x_stem_train, y_stem_train, x_stem_test, y_stem_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi : 0.9820 \n",
      "\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1084\n",
      "           1       0.99      0.97      0.98      1085\n",
      "\n",
      "    accuracy                           0.98      2169\n",
      "   macro avg       0.98      0.98      0.98      2169\n",
      "weighted avg       0.98      0.98      0.98      2169\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1075    9]\n",
      " [  30 1055]]\n"
     ]
    }
   ],
   "source": [
    "latih(RandomForestClassifier(), x_stem_train_smote,\n",
    "      y_stem_train_smote, x_stem_test_smote, y_stem_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 100} \n",
      "\n",
      "Akurasi : 0.9435\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.96      1084\n",
      "           1       0.98      0.76      0.86       315\n",
      "\n",
      "    accuracy                           0.94      1399\n",
      "   macro avg       0.96      0.88      0.91      1399\n",
      "weighted avg       0.95      0.94      0.94      1399\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1080    4]\n",
      " [  75  240]]\n"
     ]
    }
   ],
   "source": [
    "latih_hyperparameter(RandomForestClassifier(), x_stem_train,\n",
    "                     y_stem_train, x_stem_test, y_stem_test, param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 400} \n",
      "\n",
      "Akurasi : 0.9723\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1084\n",
      "           1       0.99      0.95      0.97      1085\n",
      "\n",
      "    accuracy                           0.97      2169\n",
      "   macro avg       0.97      0.97      0.97      2169\n",
      "weighted avg       0.97      0.97      0.97      2169\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1075    9]\n",
      " [  51 1034]]\n"
     ]
    }
   ],
   "source": [
    "latih_hyperparameter(RandomForestClassifier(), x_stem_train_smote,\n",
    "                     y_stem_train_smote, x_stem_test_smote, y_stem_test_smote, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling 4 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi untuk PORNOGRAFI : 0.95\n",
      "Hasil rata-rata akurasi Cross Validation : 0.9472826503255458\n",
      "Rata-rata std Cross Validation : 0.004905372194183675\n",
      "Akurasi untuk SARA : 0.90\n",
      "Hasil rata-rata akurasi Cross Validation : 0.8833109919571045\n",
      "Rata-rata std Cross Validation : 0.006019581074792571\n",
      "Akurasi untuk RADIKALISME : 0.94\n",
      "Hasil rata-rata akurasi Cross Validation : 0.9240535235541938\n",
      "Rata-rata std Cross Validation : 0.008984405713702948\n",
      "Akurasi untuk PENCEMARAN_NAMA_BAIK : 0.81\n",
      "Hasil rata-rata akurasi Cross Validation : 0.8080754500191498\n",
      "Rata-rata std Cross Validation : 0.011847323725986092\n"
     ]
    }
   ],
   "source": [
    "# model = RandomForestClassifier()\n",
    "\n",
    "# for label in labels:\n",
    "#     y = df[label]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         x, y, stratify=y, test_size=0.2, random_state=32)\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     print(f'Akurasi untuk {label} : {accuracy_score(y_test, y_pred):,.2f}')\n",
    "\n",
    "#     Skfold = StratifiedKFold(n_splits=5)\n",
    "#     acc = cross_val_score(model, X_train, y_train, cv=Skfold)\n",
    "#     print(f'Hasil rata-rata akurasi Cross Validation : {acc.mean()}')\n",
    "#     print(f'Rata-rata std Cross Validation : {acc.std()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woi anjing bodoh provinsi paling banyak ngerus...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saja nitizen penasaran keluarga besar sudah pa...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok semoga sipenista agama ateknya mati...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang tetap dibedakan ko...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buat anak melulu kamu nof mikir apa kasian ana...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     source  PORNOGRAFI  \\\n",
       "0  woi anjing bodoh provinsi paling banyak ngerus...     kaskus           0   \n",
       "1  saja nitizen penasaran keluarga besar sudah pa...  instagram           0   \n",
       "2  sidangahok semoga sipenista agama ateknya mati...    twitter           0   \n",
       "3  jakarta barusan baca undang tetap dibedakan ko...  instagram           0   \n",
       "4  buat anak melulu kamu nof mikir apa kasian ana...     kaskus           0   \n",
       "\n",
       "   SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0     0            0                     1  \n",
       "1     0            0                     0  \n",
       "2     1            1                     1  \n",
       "3     0            0                     0  \n",
       "4     0            0                     0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kata = pd.read_csv('data hasil/data_kalimat.csv')\n",
    "df_kata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                    0\n",
       "source                  0\n",
       "PORNOGRAFI              0\n",
       "SARA                    0\n",
       "RADIKALISME             0\n",
       "PENCEMARAN_NAMA_BAIK    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kata = df_kata.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woi anjing bodoh provinsi paling banyak ngerus...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saja nitizen penasaran keluarga besar sudah pa...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok semoga sipenista agama ateknya mati...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang tetap dibedakan ko...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buat anak melulu kamu nof mikir apa kasian ana...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>mas sebenarnya aku mau buly mas seperti nya ma...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>kdang mengaku nicky minaj kdang beyonce kdang ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>waktu kemas tadi terfikir jugak botol air keci...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>heh kontol jan sok pemes kamu rp muka memek or...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>main bacok an ayo quenmutia kalau kamu suka bu...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6991 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     source  \\\n",
       "0     woi anjing bodoh provinsi paling banyak ngerus...     kaskus   \n",
       "1     saja nitizen penasaran keluarga besar sudah pa...  instagram   \n",
       "2     sidangahok semoga sipenista agama ateknya mati...    twitter   \n",
       "3     jakarta barusan baca undang tetap dibedakan ko...  instagram   \n",
       "4     buat anak melulu kamu nof mikir apa kasian ana...     kaskus   \n",
       "...                                                 ...        ...   \n",
       "6986  mas sebenarnya aku mau buly mas seperti nya ma...  instagram   \n",
       "6987  kdang mengaku nicky minaj kdang beyonce kdang ...  instagram   \n",
       "6988  waktu kemas tadi terfikir jugak botol air keci...    twitter   \n",
       "6989  heh kontol jan sok pemes kamu rp muka memek or...    twitter   \n",
       "6990  main bacok an ayo quenmutia kalau kamu suka bu...    twitter   \n",
       "\n",
       "      PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK  \n",
       "0              0     0            0                     1  \n",
       "1              0     0            0                     0  \n",
       "2              0     1            1                     1  \n",
       "3              0     0            0                     0  \n",
       "4              0     0            0                     0  \n",
       "...          ...   ...          ...                   ...  \n",
       "6986           0     0            0                     0  \n",
       "6987           0     0            0                     0  \n",
       "6988           0     0            1                     0  \n",
       "6989           1     0            0                     1  \n",
       "6990           0     1            1                     1  \n",
       "\n",
       "[6991 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_kata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_kata\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdata hasil/data_kalimat.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_kata' is not defined"
     ]
    }
   ],
   "source": [
    "df_kata.to_csv('data hasil/data_kalimat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 200} \n",
      "\n",
      "Akurasi : 0.9640\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1060\n",
      "           1       0.99      0.94      0.96      1109\n",
      "\n",
      "    accuracy                           0.96      2169\n",
      "   macro avg       0.97      0.96      0.96      2169\n",
      "weighted avg       0.97      0.96      0.96      2169\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1053    7]\n",
      " [  71 1038]]\n"
     ]
    }
   ],
   "source": [
    "best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\n",
    "                                  y_train_smote, x_test_smote, y_test_smote, param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=100, min_samples_leaf=3, min_samples_split=10,\n",
       "                       n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=100, min_samples_leaf=3, min_samples_split=10,\n",
       "                       n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=100, min_samples_leaf=3, min_samples_split=10,\n",
       "                       n_estimators=200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yang',\n",
       " 'untuk',\n",
       " 'pada',\n",
       " 'ke',\n",
       " 'para',\n",
       " 'namun',\n",
       " 'menurut',\n",
       " 'antara',\n",
       " 'dia',\n",
       " 'dua',\n",
       " 'ia',\n",
       " 'seperti',\n",
       " 'jika',\n",
       " 'jika',\n",
       " 'sehingga',\n",
       " 'kembali',\n",
       " 'dan',\n",
       " 'tidak',\n",
       " 'ini',\n",
       " 'karena',\n",
       " 'kepada',\n",
       " 'oleh',\n",
       " 'saat',\n",
       " 'harus',\n",
       " 'sementara',\n",
       " 'setelah',\n",
       " 'belum',\n",
       " 'kami',\n",
       " 'sekitar',\n",
       " 'bagi',\n",
       " 'serta',\n",
       " 'di',\n",
       " 'dari',\n",
       " 'telah',\n",
       " 'sebagai',\n",
       " 'masih',\n",
       " 'hal',\n",
       " 'ketika',\n",
       " 'adalah',\n",
       " 'itu',\n",
       " 'dalam',\n",
       " 'bisa',\n",
       " 'bahwa',\n",
       " 'atau',\n",
       " 'hanya',\n",
       " 'kita',\n",
       " 'dengan',\n",
       " 'akan',\n",
       " 'juga',\n",
       " 'ada',\n",
       " 'mereka',\n",
       " 'sudah',\n",
       " 'saya',\n",
       " 'terhadap',\n",
       " 'secara',\n",
       " 'agar',\n",
       " 'lain',\n",
       " 'anda',\n",
       " 'begitu',\n",
       " 'mengapa',\n",
       " 'kenapa',\n",
       " 'yaitu',\n",
       " 'yakni',\n",
       " 'daripada',\n",
       " 'itulah',\n",
       " 'lagi',\n",
       " 'maka',\n",
       " 'tentang',\n",
       " 'demi',\n",
       " 'dimana',\n",
       " 'kemana',\n",
       " 'pula',\n",
       " 'sambil',\n",
       " 'sebelum',\n",
       " 'sesudah',\n",
       " 'supaya',\n",
       " 'guna',\n",
       " 'kah',\n",
       " 'pun',\n",
       " 'sampai',\n",
       " 'sedangkan',\n",
       " 'selagi',\n",
       " 'sementara',\n",
       " 'tetapi',\n",
       " 'apakah',\n",
       " 'kecuali',\n",
       " 'sebab',\n",
       " 'selain',\n",
       " 'seolah',\n",
       " 'seraya',\n",
       " 'seterusnya',\n",
       " 'tanpa',\n",
       " 'agak',\n",
       " 'boleh',\n",
       " 'dapat',\n",
       " 'dsb',\n",
       " 'dst',\n",
       " 'dll',\n",
       " 'dahulu',\n",
       " 'dulunya',\n",
       " 'anu',\n",
       " 'demikian',\n",
       " 'tapi',\n",
       " 'ingin',\n",
       " 'juga',\n",
       " 'nggak',\n",
       " 'mari',\n",
       " 'nanti',\n",
       " 'melainkan',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'seharusnya',\n",
       " 'sebetulnya',\n",
       " 'setiap',\n",
       " 'setidaknya',\n",
       " 'sesuatu',\n",
       " 'pasti',\n",
       " 'saja',\n",
       " 'toh',\n",
       " 'ya',\n",
       " 'walau',\n",
       " 'tolong',\n",
       " 'tentu',\n",
       " 'amat',\n",
       " 'apalagi',\n",
       " 'bagaimanapun']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeFactory.get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=removeFactory.get_stop_words(), analyzer='word')\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('tf-idf', tfidf),\n",
    "    ('model', best_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&#x27;yang&#x27;, &#x27;untuk&#x27;, &#x27;pada&#x27;, &#x27;ke&#x27;,\n",
       "                                             &#x27;para&#x27;, &#x27;namun&#x27;, &#x27;menurut&#x27;,\n",
       "                                             &#x27;antara&#x27;, &#x27;dia&#x27;, &#x27;dua&#x27;, &#x27;ia&#x27;,\n",
       "                                             &#x27;seperti&#x27;, &#x27;jika&#x27;, &#x27;jika&#x27;,\n",
       "                                             &#x27;sehingga&#x27;, &#x27;kembali&#x27;, &#x27;dan&#x27;,\n",
       "                                             &#x27;tidak&#x27;, &#x27;ini&#x27;, &#x27;karena&#x27;, &#x27;kepada&#x27;,\n",
       "                                             &#x27;oleh&#x27;, &#x27;saat&#x27;, &#x27;harus&#x27;,\n",
       "                                             &#x27;sementara&#x27;, &#x27;setelah&#x27;, &#x27;belum&#x27;,\n",
       "                                             &#x27;kami&#x27;, &#x27;sekitar&#x27;, &#x27;bagi&#x27;, ...])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(max_depth=100, min_samples_leaf=3,\n",
       "                                        min_samples_split=10,\n",
       "                                        n_estimators=200))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&#x27;yang&#x27;, &#x27;untuk&#x27;, &#x27;pada&#x27;, &#x27;ke&#x27;,\n",
       "                                             &#x27;para&#x27;, &#x27;namun&#x27;, &#x27;menurut&#x27;,\n",
       "                                             &#x27;antara&#x27;, &#x27;dia&#x27;, &#x27;dua&#x27;, &#x27;ia&#x27;,\n",
       "                                             &#x27;seperti&#x27;, &#x27;jika&#x27;, &#x27;jika&#x27;,\n",
       "                                             &#x27;sehingga&#x27;, &#x27;kembali&#x27;, &#x27;dan&#x27;,\n",
       "                                             &#x27;tidak&#x27;, &#x27;ini&#x27;, &#x27;karena&#x27;, &#x27;kepada&#x27;,\n",
       "                                             &#x27;oleh&#x27;, &#x27;saat&#x27;, &#x27;harus&#x27;,\n",
       "                                             &#x27;sementara&#x27;, &#x27;setelah&#x27;, &#x27;belum&#x27;,\n",
       "                                             &#x27;kami&#x27;, &#x27;sekitar&#x27;, &#x27;bagi&#x27;, ...])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(max_depth=100, min_samples_leaf=3,\n",
       "                                        min_samples_split=10,\n",
       "                                        n_estimators=200))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;yang&#x27;, &#x27;untuk&#x27;, &#x27;pada&#x27;, &#x27;ke&#x27;, &#x27;para&#x27;, &#x27;namun&#x27;,\n",
       "                            &#x27;menurut&#x27;, &#x27;antara&#x27;, &#x27;dia&#x27;, &#x27;dua&#x27;, &#x27;ia&#x27;, &#x27;seperti&#x27;,\n",
       "                            &#x27;jika&#x27;, &#x27;jika&#x27;, &#x27;sehingga&#x27;, &#x27;kembali&#x27;, &#x27;dan&#x27;,\n",
       "                            &#x27;tidak&#x27;, &#x27;ini&#x27;, &#x27;karena&#x27;, &#x27;kepada&#x27;, &#x27;oleh&#x27;, &#x27;saat&#x27;,\n",
       "                            &#x27;harus&#x27;, &#x27;sementara&#x27;, &#x27;setelah&#x27;, &#x27;belum&#x27;, &#x27;kami&#x27;,\n",
       "                            &#x27;sekitar&#x27;, &#x27;bagi&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=100, min_samples_leaf=3, min_samples_split=10,\n",
       "                       n_estimators=200)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tf-idf',\n",
       "                 TfidfVectorizer(stop_words=['yang', 'untuk', 'pada', 'ke',\n",
       "                                             'para', 'namun', 'menurut',\n",
       "                                             'antara', 'dia', 'dua', 'ia',\n",
       "                                             'seperti', 'jika', 'jika',\n",
       "                                             'sehingga', 'kembali', 'dan',\n",
       "                                             'tidak', 'ini', 'karena', 'kepada',\n",
       "                                             'oleh', 'saat', 'harus',\n",
       "                                             'sementara', 'setelah', 'belum',\n",
       "                                             'kami', 'sekitar', 'bagi', ...])),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_depth=100, min_samples_leaf=3,\n",
       "                                        min_samples_split=10,\n",
       "                                        n_estimators=200))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(df_kata['text'], df_kata['PORNOGRAFI'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'hahaha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.predict([kalimat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95295063, 0.04704937]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.predict_proba([kalimat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pornografi', 'wb') as picklefile:\n",
    "    pickle.dump(pl,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pipe(model, x, y, stop_words):\n",
    "#     tfidf = TfidfVectorizer(\n",
    "#         stop_words=stop_words, analyzer='word')\n",
    "\n",
    "#     pl = Pipeline([\n",
    "#         ('tf-idf', tfidf),\n",
    "#         ('model', model)\n",
    "#     ])\n",
    "\n",
    "#     pl.fit(x, df[y])\n",
    "#     with open(f'model {y}', 'wb') as picklefile:\n",
    "#         pickle.dump(pl,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PORNOGRAFI</th>\n",
       "      <th>SARA</th>\n",
       "      <th>RADIKALISME</th>\n",
       "      <th>PENCEMARAN_NAMA_BAIK</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abadinya</th>\n",
       "      <th>abah</th>\n",
       "      <th>abai</th>\n",
       "      <th>...</th>\n",
       "      <th>zuhur</th>\n",
       "      <th>zulkarnaen</th>\n",
       "      <th>zulkarnain</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zuqfaiugqy</th>\n",
       "      <th>zvjqohw</th>\n",
       "      <th>zwcxnxhd</th>\n",
       "      <th>zwnbzkjn</th>\n",
       "      <th>zy</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25708 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PORNOGRAFI  SARA  RADIKALISME  PENCEMARAN_NAMA_BAIK   ab  abad  abadi  \\\n",
       "0           0     0            0                     1  0.0   0.0    0.0   \n",
       "1           0     0            0                     0  0.0   0.0    0.0   \n",
       "2           0     1            1                     1  0.0   0.0    0.0   \n",
       "3           0     0            0                     0  0.0   0.0    0.0   \n",
       "4           0     0            0                     0  0.0   0.0    0.0   \n",
       "\n",
       "   abadinya  abah  abai  ...  zuhur  zulkarnaen  zulkarnain  zulkifli  \\\n",
       "0       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "1       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "2       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "3       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "4       0.0   0.0   0.0  ...    0.0         0.0         0.0       0.0   \n",
       "\n",
       "   zuqfaiugqy  zvjqohw  zwcxnxhd  zwnbzkjn   zy  zynga  \n",
       "0         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "1         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "2         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "3         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "4         0.0      0.0       0.0       0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 25708 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import itertools\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "labels = ['SARA', 'RADIKALISME', 'PENCEMARAN_NAMA_BAIK', 'PORNOGRAFI']\n",
    "\n",
    "\n",
    "# factory = StopWordRemoverFactory()\n",
    "# stopwords = factory.get_stop_words()\n",
    "\n",
    "removeFactory = StopWordRemoverFactory()\n",
    "stopwords = removeFactory.create_stop_word_remover()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "df = pd.read_csv('data hasil/preprocess_wo_stemming_fix_backslash.csv')\n",
    "df_kata = pd.read_csv('data hasil/data_kalimat.csv')\n",
    "\n",
    "x = df.drop(columns=['PORNOGRAFI', 'SARA',\n",
    "            'RADIKALISME', 'PENCEMARAN_NAMA_BAIK'])\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "\n",
    "def latih(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(f'Akurasi : {accuracy_score(y_test, y_pred):,.4f} \\n')\n",
    "    print('---------- Classification Report ----------')\n",
    "    print(f'{classification_report(y_test, y_pred)}\\n')\n",
    "    print('---------- Confusion Matrix ----------')\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    print(conf)\n",
    "\n",
    "\n",
    "def latih_hyperparameter(model, x_train, y_train, x_test, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                               cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(f'Parameter terbaik : {grid_search.best_params_} \\n')\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    best_grid.fit(x_train, y_train)\n",
    "    y_pred = best_grid.predict(x_test)\n",
    "    print(f'Akurasi : {accuracy_score(y_test, y_pred):,.4f}')\n",
    "    print('---------- Classification Report ----------')\n",
    "    print(f'{classification_report(y_test, y_pred)}\\n')\n",
    "    print('---------- Confusion Matrix ----------')\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    print(conf)\n",
    "\n",
    "    return best_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pipeline_model(x, y, model, stopwords):\n",
    "#     y = df[label]\n",
    "#     x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "\n",
    "#     x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "#         x_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "#     best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\n",
    "#                                       y_train_smote, x_test_smote, y_test_smote, param_grid)\n",
    "\n",
    "#     tfidf = TfidfVectorizer(\n",
    "#         stop_words=removeFactory.get_stop_words(), analyzer='word')\n",
    "\n",
    "#     pl = Pipeline([\n",
    "#         ('tf-idf', tfidf),\n",
    "#         ('model', best_model)\n",
    "#     ])\n",
    "\n",
    "#     pl.fit(df_kata['text'], df_kata[label])\n",
    "\n",
    "#     pl = Pipeline([\n",
    "#         ('tf-idf', tfidf),\n",
    "#         ('model', best_model)\n",
    "#     ])\n",
    "\n",
    "#     with open(f'data hasil/{label}', 'wb') as picklefile:\n",
    "#         pickle.dump(pl, picklefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SARA ==========\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 100} \n",
      "\n",
      "Akurasi : 0.8563\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92      1167\n",
      "           1       0.79      0.18      0.29       232\n",
      "\n",
      "    accuracy                           0.86      1399\n",
      "   macro avg       0.83      0.59      0.61      1399\n",
      "weighted avg       0.85      0.86      0.82      1399\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1156   11]\n",
      " [ 190   42]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 22\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# x_smote, y_smote = oversample.fit_resample(x, y)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[39m# x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#                                   y_train_smote, x_test_smote, y_test_smote, param_grid)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m     20\u001b[0m     stop_words\u001b[39m=\u001b[39mremoveFactory\u001b[39m.\u001b[39mget_stop_words(), analyzer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m pl \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     23\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mtf-idf\u001b[39m\u001b[39m'\u001b[39m, tfidf),\n\u001b[0;32m     24\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, best_model)\n\u001b[0;32m     25\u001b[0m ])\n\u001b[0;32m     27\u001b[0m pl\u001b[39m.\u001b[39mfit(df_kata[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], df_kata[label])\n\u001b[0;32m     29\u001b[0m pl \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     30\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mtf-idf\u001b[39m\u001b[39m'\u001b[39m, tfidf),\n\u001b[0;32m     31\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, best_model)\n\u001b[0;32m     32\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(f'========== {label} ==========')\n",
    "\n",
    "    \n",
    "    y = df[label]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_model = latih_hyperparameter(RandomForestClassifier(), x_train,\n",
    "                                      y_train, x_test, y_test, param_grid)\n",
    "    \n",
    "    # x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "    \n",
    "    # x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "    #     x_smote, y_smote, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\n",
    "    #                                   y_train_smote, x_test_smote, y_test_smote, param_grid)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=removeFactory.get_stop_words(), analyzer='word')\n",
    "\n",
    "    pl = Pipeline([\n",
    "        ('tf-idf', tfidf),\n",
    "        ('model', best_model)\n",
    "    ])\n",
    "    \n",
    "    pl.fit(df_kata['text'], df_kata[label])\n",
    "\n",
    "    pl = Pipeline([\n",
    "        ('tf-idf', tfidf),\n",
    "        ('model', best_model)\n",
    "    ])\n",
    "    \n",
    "    with open(f'model/no smote {label}', 'wb') as picklefile:\n",
    "        pickle.dump(pl, picklefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SARA ==========\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 90, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 400} \n",
      "\n",
      "Akurasi : 0.9408\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1189\n",
      "           1       0.92      0.96      0.94      1160\n",
      "\n",
      "    accuracy                           0.94      2349\n",
      "   macro avg       0.94      0.94      0.94      2349\n",
      "weighted avg       0.94      0.94      0.94      2349\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1098   91]\n",
      " [  48 1112]]\n",
      "========== RADIKALISME ==========\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Parameter terbaik : {'bootstrap': True, 'max_depth': 90, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 200} \n",
      "\n",
      "Akurasi : 0.9499\n",
      "---------- Classification Report ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95      1202\n",
      "           1       0.92      0.98      0.95      1135\n",
      "\n",
      "    accuracy                           0.95      2337\n",
      "   macro avg       0.95      0.95      0.95      2337\n",
      "weighted avg       0.95      0.95      0.95      2337\n",
      "\n",
      "\n",
      "---------- Confusion Matrix ----------\n",
      "[[1103   99]\n",
      " [  18 1117]]\n",
      "========== PENCEMARAN_NAMA_BAIK ==========\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(f'========== {label} ==========')\n",
    "\n",
    "    \n",
    "    y = df[label]\n",
    "    x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "    \n",
    "    x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "        x_smote, y_smote, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_model = latih_hyperparameter(RandomForestClassifier(), x_train_smote,\n",
    "                                      y_train_smote, x_test_smote, y_test_smote, param_grid)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=removeFactory.get_stop_words(), analyzer='word')\n",
    "\n",
    "    pl = Pipeline([\n",
    "        ('tf-idf', tfidf),\n",
    "        ('model', best_model)\n",
    "    ])\n",
    "    \n",
    "    pl.fit(df_kata['text'], df_kata[label])\n",
    "\n",
    "    pl = Pipeline([\n",
    "        ('tf-idf', tfidf),\n",
    "        ('model', best_model)\n",
    "    ])\n",
    "    \n",
    "    with open(f'data hasil/{label}', 'wb') as picklefile:\n",
    "        pickle.dump(pl, picklefile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fddeb631dcdb91a2009415ee4ac7fa78a8d2991534154dee406a6e242d84dfd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
